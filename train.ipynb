{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e598d32-ae16-4237-9fa8-24c25a332fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U\n",
    "!pip install matplotlib datasets transformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fa21a-0460-4f48-b611-815621c2d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U\n",
    "# !pip install matplotlib datasets transformers wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import argparse\n",
    "from huggingface_hub import HfFolder, Repository, login\n",
    "import os\n",
    "\n",
    "def split_train_test(dataset, train_ratio=0.9):\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    return random_split(dataset, [train_size, test_size])\n",
    "\n",
    "def configure_trainer_instance(model, tokenizer, train_dataset, test_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=64,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=1,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=128,\n",
    "        fp16=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def plot_training_metrics(trainer, metric_name):\n",
    "    metric_values = [log[metric_name] for log in trainer.state.log_history if metric_name in log]\n",
    "    plt.plot(metric_values, label=metric_name)\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(metric_name.capitalize())\n",
    "    plt.title(f\"{metric_name.capitalize()} vs Training Steps\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_gpt2_model(trainer):\n",
    "    return trainer.evaluate()\n",
    "\n",
    "def create_gpt2_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", cache_dir=\"/workspace/cache\")\n",
    "    # Ensure all special tokens are set\n",
    "    special_tokens_dict = {\n",
    "        'bos_token': tokenizer.bos_token,\n",
    "        'eos_token': tokenizer.eos_token,\n",
    "        'pad_token': tokenizer.eos_token,\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    config = GPT2Config(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        n_positions=2048,\n",
    "        n_embd=2048,\n",
    "        n_layer=16,\n",
    "        n_head=16,\n",
    "        activation_function=\"gelu_new\",\n",
    "        resid_pdrop=0.1,\n",
    "        embd_pdrop=0.1,\n",
    "        attn_pdrop=0.1,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        initializer_range=0.02,\n",
    "        scale_attn_weights=True,\n",
    "        use_cache=True,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    model = GPT2LMHeadModel(config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.init_weights()\n",
    "    return tokenizer, model\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"longest\", max_length=2048)\n",
    "\n",
    "def print_model_info(model, tokenizer):\n",
    "    print(\"\\n===== Model Information =====\")\n",
    "    print(f\"Model Type: {type(model).__name__}\")\n",
    "    print(f\"Number of Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Number of Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    print(\"\\n--- Model Architecture ---\")\n",
    "    print(model)\n",
    "    \n",
    "    print(\"\\n--- Model Config ---\")\n",
    "    for key, value in model.config.to_dict().items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nVocabulary Size: {len(tokenizer)}\")\n",
    "    print(f\"Model Max Length: {model.config.n_positions}\")\n",
    "    print(f\"Embedding Size: {model.config.n_embd}\")\n",
    "    print(f\"Number of Layers: {model.config.n_layer}\")\n",
    "    print(f\"Number of Attention Heads: {model.config.n_head}\")\n",
    "\n",
    "    print(\"\\n--- Special Tokens ---\")\n",
    "    print(f\"BOS Token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "    print(f\"EOS Token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "    print(f\"PAD Token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    \n",
    "    print(\"\\n===== End of Model Information =====\\n\")\n",
    "\n",
    "def push_to_hub(model, tokenizer, repo_name):\n",
    "    print(f\"Pushing model and tokenizer to Hub repository: {repo_name}\")\n",
    "    model.push_to_hub(repo_name)\n",
    "    tokenizer.push_to_hub(repo_name)\n",
    "    print(\"Model and tokenizer successfully pushed to Hub!\")\n",
    "\n",
    "login(token=\"HF_TOKEN_HERE\")\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"gpt2-training\", name=\"gpt2-llama3-tokenizer\")\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer, model = create_gpt2_model()\n",
    "\n",
    "# Print model information\n",
    "print_model_info(model, tokenizer)\n",
    "\n",
    "# Load and tokenize the dataset\n",
    "dataset = load_dataset(\"dustinwloring1988/fineweb-edu-test-small-sample\", split=\"train\", cache_dir=\"/workspace/cache\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Train-test split\n",
    "train_dataset, test_dataset = split_train_test(tokenized_dataset)\n",
    "\n",
    "# Configure Trainer instance\n",
    "trainer = configure_trainer_instance(model, tokenizer, train_dataset, test_dataset)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = evaluate_gpt2_model(trainer)\n",
    "print(\"Evaluation Results:\")\n",
    "print(eval_results)\n",
    "\n",
    "# Log final evaluation results to wandb\n",
    "wandb.log({\"final_eval\": eval_results})\n",
    "\n",
    "# Visualize the model performance\n",
    "plot_training_metrics(trainer, \"loss\")\n",
    "\n",
    "# Save the model locally\n",
    "model.save_pretrained(\"./output/final_model\")\n",
    "tokenizer.save_pretrained(\"./output/final_model\")\n",
    "\n",
    "# Push to Hub if flag is set\n",
    "push_to_hub(model, tokenizer, \"dustinwloring1988/fineweb-edu-new-test-delete-2\")\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04292ae3-f916-4ef7-bc67-fd8b0deb5289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
